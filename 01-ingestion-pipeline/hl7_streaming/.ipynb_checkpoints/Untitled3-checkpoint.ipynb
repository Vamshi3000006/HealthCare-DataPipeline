{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca84fe4f-df1e-496d-acbd-111d16702553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HL7 Reader\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c14b95-c5a8-416a-9771-b6e7c2052d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a645aa00-c368-48d7-92c5-dddbf241e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b736cce0-c8ab-49dc-9acf-3a92aec17b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HL7 Kafka to Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 4: Read Kafka HL7 messages\n",
    "data = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"hl7-events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Step 5: Cast Kafka value to string\n",
    "raw_df = data.selectExpr(\"CAST(value AS STRING) AS raw_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bb5f35-914b-4a65-9922-498633481d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccd555e-658f-48bd-80be-9b91ec5078a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hl7_to_standard_json(message):\n",
    "    try:\n",
    "        print(\"⚠️ RAW HL7 INPUT:\", message)\n",
    "\n",
    "        # Normalize line breaks\n",
    "        lines = message.replace('\\r', '\\n').split('\\n')\n",
    "        data = {}\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split('|')\n",
    "            if not parts or len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            seg_type = parts[0].strip()\n",
    "\n",
    "            if seg_type == \"PID\":\n",
    "                data[\"patient_id\"] = parts[3] if len(parts) > 3 else \"\"\n",
    "                name_parts = parts[5].split(\"^\") if len(parts) > 5 else []\n",
    "                data[\"last_name\"] = name_parts[0] if len(name_parts) > 0 else \"\"\n",
    "                data[\"first_name\"] = name_parts[1] if len(name_parts) > 1 else \"\"\n",
    "                data[\"dob\"] = parts[7] if len(parts) > 7 else \"\"\n",
    "                data[\"gender\"] = parts[8] if len(parts) > 8 else \"\"\n",
    "\n",
    "            elif seg_type == \"PV1\":\n",
    "                data[\"provider_id\"] = parts[7] if len(parts) > 7 else \"\"\n",
    "                data[\"sending_facility\"] = parts[3] if len(parts) > 3 else \"\"\n",
    "\n",
    "            elif seg_type == \"DG1\":\n",
    "                data[\"primary_diagnosis_code\"] = parts[3] if len(parts) > 3 else \"\"\n",
    "\n",
    "            elif seg_type == \"IN1\" and \"claim_id\" not in data:\n",
    "                data[\"claim_id\"] = parts[36] if len(parts) > 36 else \"\"\n",
    "                data[\"payer_name\"] = parts[4] if len(parts) > 4 else \"\"\n",
    "                data[\"message_datetime\"] = parts[12] if len(parts) > 12 else \"\"\n",
    "\n",
    "            elif seg_type == \"IN2\" and \"claim_start_date\" not in data:\n",
    "                data[\"claim_start_date\"] = parts[2] if len(parts) > 2 else \"\"\n",
    "                data[\"claim_end_date\"] = parts[3] if len(parts) > 3 else \"\"\n",
    "\n",
    "            elif seg_type == \"MSH\":\n",
    "                data[\"sending_facility\"] = parts[4] if len(parts) > 4 else \"\"\n",
    "                data[\"message_datetime\"] = parts[6] if len(parts) > 6 else \"\"\n",
    "\n",
    "        # Add missing fields\n",
    "        required_fields = [\n",
    "            \"sending_facility\", \"message_datetime\", \"patient_id\",\n",
    "            \"last_name\", \"first_name\", \"dob\", \"gender\",\n",
    "            \"provider_id\", \"primary_diagnosis_code\", \"claim_id\",\n",
    "            \"payer_name\", \"claim_start_date\", \"claim_end_date\"\n",
    "        ]\n",
    "        for key in required_fields:\n",
    "            data.setdefault(key, \"\")\n",
    "\n",
    "        return json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)}, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e652ee1e-39f7-4bac-a609-68935c28a925",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (285305229.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    .config(\"spark.hadoop.io.native.lib\", \"false\") \\\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "parse_udf = udf(hl7_to_standard_json, StringType())\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HL7 Kafka to Delta\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.4,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ✅ THIS LINE FIXES THE BOOLEAN ERROR\n",
    "\n",
    "# Step 4: Read Kafka HL7 messages\n",
    "data = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"hl7-events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Step 5: Cast Kafka value to string\n",
    "raw_df = data.selectExpr(\"CAST(value AS STRING) AS raw_str\")\n",
    "\n",
    "# Step 6: Apply custom parser to convert raw HL7 into standardized JSON\n",
    "parsed_df = raw_df.withColumn(\"parsed_json\", parse_udf(col(\"raw_str\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4563fbf-fe57-4621-b263-3daa39085a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"hl7-events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cc4a5-bca8-40d0-b8b9-14beba9b5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df.select(\"parsed_json\").writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
